{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":482,"sourceType":"datasetVersion","datasetId":228}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Healthcare – Early Detection of Diabetes Using Machine Learning\n\n## Background\n\nA mobile health clinic aims to pre-screen patients for diabetes using basic health indicators like glucose level, BMI, insulin levels, and age. The goal is to:\n\n1. Reduce hospital crowding,\n2. Prioritize care for at-risk individuals,\n3. Enable early intervention to prevent complications.\n\n## Problem statement\n\nMany people remain undiagnosed or are diagnosed too late with diabetes, especially Type 2 Diabetes. Traditional screening methods are resource-intensive. There is a need to:\n\n* Predict the likelihood of diabetes before formal testing\n* Use machine learning (ML) to classify whether a person is likely diabetic based on easily measurable indicators.\n\n## Objective\n\nTo build a classification model that predicts whether a person has diabetes or not, based on features such as BMI, glucose level, insulin level, age, and others from the Pima Indians Diabetes dataset.\n\n## Preparing the environment\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"## Import all necessary libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import resample\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier #Added for a more robust model option\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\nfrom xgboost import XGBClassifier\nfrom sklearn.impute import SimpleImputer\nimport warnings\nwarnings.filterwarnings('ignore')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:33:47.457191Z","iopub.execute_input":"2025-06-13T06:33:47.457585Z","iopub.status.idle":"2025-06-13T06:33:47.468516Z","shell.execute_reply.started":"2025-06-13T06:33:47.457558Z","shell.execute_reply":"2025-06-13T06:33:47.466719Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## importing the dataset\ndf = pd.read_csv(\"/kaggle/input/pima-indians-diabetes-database/diabetes.csv\")\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:33:47.470253Z","iopub.execute_input":"2025-06-13T06:33:47.470590Z","iopub.status.idle":"2025-06-13T06:33:47.531256Z","shell.execute_reply.started":"2025-06-13T06:33:47.470561Z","shell.execute_reply":"2025-06-13T06:33:47.529515Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extract columns as a data frame.\n\ncols = df.columns\ncolumns = pd.DataFrame(cols, columns=['Column Names'])\ncolumns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:33:47.533225Z","iopub.execute_input":"2025-06-13T06:33:47.533582Z","iopub.status.idle":"2025-06-13T06:33:47.547986Z","shell.execute_reply.started":"2025-06-13T06:33:47.533556Z","shell.execute_reply":"2025-06-13T06:33:47.546143Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Description of the columns\n\n| Column Name              | Description                                                      |\n| ------------------------ | ---------------------------------------------------------------- |\n| Pregnancies              | Number of times pregnant                                         |\n| Glucose                  | Plasma glucose concentration                                     |\n| BloodPressure            | Diastolic blood pressure (mm Hg)                                 |\n| SkinThickness            | Triceps skinfold thickness (mm)                                  |\n| Insulin                  | 2-Hour serum insulin (mu U/ml)                                   |\n| BMI                      | Body mass index (weight in kg/m²)                                |\n| DiabetesPedigreeFunction | Function that scores diabetes likelihood based on family history |\n| Age                      | Age in years                                                     |\n| Outcome                  | Class variable (0: Non-diabetic, 1: Diabetic)                    |\n\n## Data cleaning and preprocessing.","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:33:47.550496Z","iopub.execute_input":"2025-06-13T06:33:47.551104Z","iopub.status.idle":"2025-06-13T06:33:47.582998Z","shell.execute_reply.started":"2025-06-13T06:33:47.551051Z","shell.execute_reply":"2025-06-13T06:33:47.581781Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:33:47.584514Z","iopub.execute_input":"2025-06-13T06:33:47.584928Z","iopub.status.idle":"2025-06-13T06:33:47.616793Z","shell.execute_reply.started":"2025-06-13T06:33:47.584896Z","shell.execute_reply":"2025-06-13T06:33:47.615349Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:33:47.619613Z","iopub.execute_input":"2025-06-13T06:33:47.620023Z","iopub.status.idle":"2025-06-13T06:33:47.678167Z","shell.execute_reply.started":"2025-06-13T06:33:47.619989Z","shell.execute_reply":"2025-06-13T06:33:47.676721Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Check for null values\ndf.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:33:47.679458Z","iopub.execute_input":"2025-06-13T06:33:47.680353Z","iopub.status.idle":"2025-06-13T06:33:47.692033Z","shell.execute_reply.started":"2025-06-13T06:33:47.680319Z","shell.execute_reply":"2025-06-13T06:33:47.689416Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.duplicated().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:33:47.693290Z","iopub.execute_input":"2025-06-13T06:33:47.693598Z","iopub.status.idle":"2025-06-13T06:33:47.727208Z","shell.execute_reply.started":"2025-06-13T06:33:47.693577Z","shell.execute_reply":"2025-06-13T06:33:47.725634Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The Data has no duplicates, neither does it have null values. ","metadata":{}},{"cell_type":"code","source":"# The dataset description mentions that 0 can be a placeholder for missing values in certain columns.\n# Columns where 0 might indicate a missing value: 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI'\ncols_to_check_for_zeros = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\nprint(f\"Counts of zeros in columns {cols_to_check_for_zeros}:\")\nfor col in cols_to_check_for_zeros:\n    zero_count = (df[col] == 0).sum()\n    print(f\"- {col}: {zero_count} zeros ({(zero_count/len(df)*100):.2f}%)\") # checking for the number of zeros and percentages","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:33:47.729093Z","iopub.execute_input":"2025-06-13T06:33:47.729432Z","iopub.status.idle":"2025-06-13T06:33:47.768782Z","shell.execute_reply.started":"2025-06-13T06:33:47.729408Z","shell.execute_reply":"2025-06-13T06:33:47.767195Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Data Quality Insight: Implausible Zero Values\n\nIn the dataset, the following medical features contain zero values:\n\n- `Glucose`\n- `BloodPressure`\n- `SkinThickness`\n- `Insulin`\n- `BMI`\n\nThese physiological measurements are **always expected to have non-zero values** in any living individual. \n\nTherefore, zero entries in these columns are likely due to **missing data or incorrect recording** rather than actual valid observations.\n\n> **Note:** These zeros should be treated as missing values during data preprocessing.\n\n","metadata":{}},{"cell_type":"code","source":"zero_cols = ['Glucose', 'BloodPressure', 'SkinThickness', 'BMI', 'Insulin']\ndf[zero_cols] = df[zero_cols].replace(0, np.nan)\ndf = pd.DataFrame(SimpleImputer(strategy='median').fit_transform(df), columns=df.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:33:47.769785Z","iopub.execute_input":"2025-06-13T06:33:47.770321Z","iopub.status.idle":"2025-06-13T06:33:47.825581Z","shell.execute_reply.started":"2025-06-13T06:33:47.770272Z","shell.execute_reply":"2025-06-13T06:33:47.823948Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Zero values in key health columns are replaced with NaN to mark them as missing.  \nMissing values are then filled using the median value of each column with SimpleImputer.","metadata":{}},{"cell_type":"markdown","source":"## Exloratory Data Analysis.","metadata":{}},{"cell_type":"code","source":"# Checking for correlation\ndf_corr = df.corr()\n\nsns.heatmap(df_corr, annot=True, cmap='coolwarm')\nplt.title('Correlation Heatmap')\nplt.savefig(\"Correlation Heatmap\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:33:47.830594Z","iopub.execute_input":"2025-06-13T06:33:47.831043Z","iopub.status.idle":"2025-06-13T06:33:48.698752Z","shell.execute_reply.started":"2025-06-13T06:33:47.831018Z","shell.execute_reply":"2025-06-13T06:33:48.696976Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###  Key Correlations from the Heatmap\n\nThe heatmap highlights the strength of linear relationships between features and the target variable `Outcome`.\n\n#### Most Important Correlations:\n\n- **Glucose ↔ Outcome (0.49)**  \n  Strongest positive correlation. Higher glucose levels are clearly associated with a higher likelihood of the condition.\n\n- **BMI ↔ Outcome (0.31)**  \n  Moderate correlation. Indicates that higher body mass index contributes to the risk.\n\n- **Age ↔ Outcome (0.24)**  \n  Older individuals show a higher tendency toward the outcome.\n\n- **Pregnancies ↔ Outcome (0.22)**  \n  Suggests that more pregnancies are modestly linked with the condition, possibly due to long-term health effects.\n\n#### ⚠️ Note:\nOther features like `Insulin`, `BloodPressure`, and `SkinThickness` have weak correlations with the outcome.\n","metadata":{}},{"cell_type":"code","source":"# Checking to see if there is a relationship between number of pregnancy and likelihood of being diagnosed with diabetes\ndf.groupby(\"Pregnancies\")[\"Outcome\"].value_counts().unstack().plot(kind=\"bar\", stacked=False)\nplt.title(\"Diabetes Outcome by Number of Pregnancies\")\nplt.xlabel(\"Number of Pregnancies\")\nplt.ylabel(\"Number of Outcomes\");\nplt.savefig(\"Bargraph\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:33:48.699837Z","iopub.execute_input":"2025-06-13T06:33:48.700262Z","iopub.status.idle":"2025-06-13T06:33:49.218992Z","shell.execute_reply.started":"2025-06-13T06:33:48.700236Z","shell.execute_reply":"2025-06-13T06:33:49.217742Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Diabetes Outcome by Number of Pregnancies\n\n- Fewer pregnancies (0–2) are associated with more non-diabetic cases.\n- As the number of pregnancies increases, the proportion of diabetic cases (orange bars) also increases.\n- Diabetic outcomes become more frequent relative to non-diabetic outcomes in women with 6 or more pregnancies.\n\n> **Conclusion:** Higher pregnancy count is associated with a higher likelihood of diabetes.\n","metadata":{}},{"cell_type":"code","source":"# Group the data\ngrouped = df.groupby(\"BloodPressure\")[\"Outcome\"].value_counts().unstack()\n\n# Plot line chart\nplt.figure(figsize=(12, 6))\nfor outcome in grouped.columns:\n    plt.plot(grouped.index, grouped[outcome], marker='o', label=f'Outcome {outcome}')\n\n# Add labels and title\nplt.title(\"Diabetes Outcome Trends by Blood Pressure\")\nplt.xlabel(\"Blood Pressure\")\nplt.ylabel(\"Count\")\nplt.legend(title='Outcome')\nplt.grid(True)\nplt.savefig(\"LineGraph\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:33:49.220271Z","iopub.execute_input":"2025-06-13T06:33:49.220565Z","iopub.status.idle":"2025-06-13T06:33:49.697044Z","shell.execute_reply.started":"2025-06-13T06:33:49.220543Z","shell.execute_reply":"2025-06-13T06:33:49.695380Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Blood Pressure & Diabetes\n- **Non-diabetics (Outcome 0):** Most have diastolic blood pressure between 60–80 mmHg, with a sharp, high peak—indicating normal, stable blood pressure.\n- **Diabetics (Outcome 1):** Show a wider, flatter distribution; more diabetics have higher blood pressure (>80 mmHg).\n- **Clinical Relevance:** Diabetes is linked to greater blood pressure variability and more cases of elevated blood pressure, increasing risk for cardiovascular complications.\n- **Key Takeaway:** Blood pressure monitoring and control are crucial in diabetes management to reduce health risks.","metadata":{}},{"cell_type":"code","source":"# Check for the age distribution\nplt.figure(figsize=(10, 6))\nsns.displot(df['Age'], bins=20, color='skyblue', edgecolor='black')\nplt.title('Distribution of Age')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.savefig(\"age distribution\")\nplt.show();","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:33:49.698978Z","iopub.execute_input":"2025-06-13T06:33:49.699422Z","iopub.status.idle":"2025-06-13T06:33:50.205539Z","shell.execute_reply.started":"2025-06-13T06:33:49.699390Z","shell.execute_reply":"2025-06-13T06:33:50.202738Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Age Distribution Explanation\n\n- **Shape:** The histogram shows a right-skewed (positively skewed) distribution.\n- **Peak:** The largest group is in the early 20s, with the highest frequency around age 21–23.\n- **Trend:** As age increases, the number of individuals decreases steadily.\n- **Older Age Groups:** There are very few individuals above age 60, and almost none above 75.\n\n### Summary\n\nMost individuals in this data set are young adults, with frequency dropping sharply as age increases. The population is concentrated in the 20–40 year age range, and older adults are underrepresented.","metadata":{}},{"cell_type":"code","source":"# Checking if the data is balanced\ndf['Outcome'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:33:50.207366Z","iopub.execute_input":"2025-06-13T06:33:50.207845Z","iopub.status.idle":"2025-06-13T06:33:50.219322Z","shell.execute_reply.started":"2025-06-13T06:33:50.207792Z","shell.execute_reply":"2025-06-13T06:33:50.217352Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- We have an imbalance dataset and Machine learning models trained on imbalanced data may become biased toward the majority class. This needs to be tackled!","metadata":{}},{"cell_type":"code","source":"# Visualize the imbalance\nsns.countplot(x='Outcome', data=df)\nplt.title('Class Distribution (Outcome)')\nplt.xlabel('Outcome (0 = No Diabetes, 1 = Diabetes)')\nplt.ylabel('Count')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:33:50.220806Z","iopub.execute_input":"2025-06-13T06:33:50.221712Z","iopub.status.idle":"2025-06-13T06:33:50.411902Z","shell.execute_reply.started":"2025-06-13T06:33:50.221641Z","shell.execute_reply":"2025-06-13T06:33:50.410810Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to resample(Oversample the minority class)\n\ndef balance_dataset(df, target_column):\n    \"\"\"\n    Balance the dataset by oversampling the minority class\n\n    \"\"\"\n    # Separate majority and minority classes\n    majority = df[df[target_column] == 0]\n    minority = df[df[target_column] == 1]\n\n    print(f\"Before Oversampling:\\n{df[target_column].value_counts()}\\n\")\n\n    # Upsample minority class\n    minority_upsampled = resample(\n        minority,\n        replace=True,  # sample with replacement\n        n_samples=len(majority),  # match majority count\n        random_state=42\n    )\n\n    # Combine majority and upsampled minority\n    df_oversampled = pd.concat([majority, minority_upsampled])\n\n    print(f\"After Oversampling:\\n{df_oversampled[target_column].value_counts()}\\n\")\n\n    # Plot class distribution before and after\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n    df[target_column].value_counts().plot(kind='bar', ax=axes[0], color=['skyblue', 'orange'])\n    axes[0].set_title('Before Oversampling')\n    axes[0].set_xlabel('Class')\n    axes[0].set_ylabel('Count')\n\n    df_oversampled[target_column].value_counts().plot(kind='bar', ax=axes[1], color=['skyblue', 'orange'])\n    axes[1].set_title('After Oversampling')\n    axes[1].set_xlabel('Class')\n    axes[1].set_ylabel('Count')\n\n    plt.tight_layout()\n    plt.savefig(\"Resampling\")\n    plt.show()\n\n    return df_oversampled","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:33:50.413024Z","iopub.execute_input":"2025-06-13T06:33:50.413339Z","iopub.status.idle":"2025-06-13T06:33:50.424918Z","shell.execute_reply.started":"2025-06-13T06:33:50.413318Z","shell.execute_reply":"2025-06-13T06:33:50.423915Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Unbalanced_data = df.copy()\nbalanced_data = balance_dataset(Unbalanced_data, 'Outcome')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:33:50.425890Z","iopub.execute_input":"2025-06-13T06:33:50.426207Z","iopub.status.idle":"2025-06-13T06:33:51.010298Z","shell.execute_reply.started":"2025-06-13T06:33:50.426178Z","shell.execute_reply":"2025-06-13T06:33:51.009206Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Selection and feature engineering\n\nFrom the heatmap above we can see that the Outcome (whether a patient has diabetes or not) is not reaaly affected by the skinThickness and Blood pressure. This can further be confirmed by doing a simple Feature Contribution Analysis. To proceed we need to ensure that all the zeros in the columns of Insulin, BloodPressure, Glucose, SkinThickness, BMI have been replaced with the median of their respective columns.","metadata":{}},{"cell_type":"code","source":"Unbalanced_data.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:33:51.011277Z","iopub.execute_input":"2025-06-13T06:33:51.011530Z","iopub.status.idle":"2025-06-13T06:33:51.056476Z","shell.execute_reply.started":"2025-06-13T06:33:51.011511Z","shell.execute_reply":"2025-06-13T06:33:51.055052Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"balanced_data.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:33:51.057885Z","iopub.execute_input":"2025-06-13T06:33:51.058216Z","iopub.status.idle":"2025-06-13T06:33:51.098574Z","shell.execute_reply.started":"2025-06-13T06:33:51.058185Z","shell.execute_reply":"2025-06-13T06:33:51.096252Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Create a copy of the balanced DataSet\ncopy_balanced = balanced_data.copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:33:51.100732Z","iopub.execute_input":"2025-06-13T06:33:51.101151Z","iopub.status.idle":"2025-06-13T06:33:51.108258Z","shell.execute_reply.started":"2025-06-13T06:33:51.101104Z","shell.execute_reply":"2025-06-13T06:33:51.106427Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"copy_balanced.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:33:51.109351Z","iopub.execute_input":"2025-06-13T06:33:51.109761Z","iopub.status.idle":"2025-06-13T06:33:51.170083Z","shell.execute_reply.started":"2025-06-13T06:33:51.109728Z","shell.execute_reply":"2025-06-13T06:33:51.168837Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x = balanced_data.drop('Outcome', axis=1)\ny = balanced_data['Outcome']\nscaler = StandardScaler()\nx_scaled = scaler.fit_transform(x)\n\n# Convert back to DataFrame\nx_scaled_df = pd.DataFrame(x_scaled, columns=x.columns)\n\n# Step 5: Reattach the Outcome column\nstandardized_data = pd.concat([x_scaled_df, y.reset_index(drop=True)], axis=1)\nstandardized_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:33:51.171044Z","iopub.execute_input":"2025-06-13T06:33:51.171322Z","iopub.status.idle":"2025-06-13T06:33:51.198167Z","shell.execute_reply.started":"2025-06-13T06:33:51.171299Z","shell.execute_reply":"2025-06-13T06:33:51.196599Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The affected columns have now been filled with the medians of their respective columns.\n\n## Feature Contribution Analysis\n\nTo carry out a feature contrinution analysis, it is necessary to extract the outcome from the other features and then fit it using Random Forest Classifier, After that features can then be arranged based on their importance.\n","metadata":{}},{"cell_type":"code","source":"X_FeatureAnalysis = copy_balanced.drop(['Outcome'], axis=1)\ny_FeatureAnalysis = copy_balanced['Outcome']\nX_FeatureAnalysis.shape, y_FeatureAnalysis.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:33:51.199335Z","iopub.execute_input":"2025-06-13T06:33:51.199724Z","iopub.status.idle":"2025-06-13T06:33:51.229194Z","shell.execute_reply.started":"2025-06-13T06:33:51.199693Z","shell.execute_reply":"2025-06-13T06:33:51.227741Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Model for feature analysis\n\nmodel_FeatureAnalysis = RandomForestClassifier(n_estimators = 100, random_state=42)\nmodel_FeatureAnalysis.fit(X_FeatureAnalysis, y_FeatureAnalysis)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:33:51.230230Z","iopub.execute_input":"2025-06-13T06:33:51.230485Z","iopub.status.idle":"2025-06-13T06:33:51.564221Z","shell.execute_reply.started":"2025-06-13T06:33:51.230465Z","shell.execute_reply":"2025-06-13T06:33:51.562613Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Importances = model_FeatureAnalysis.feature_importances_   # This gets feature importance\nfeature_importances = pd.DataFrame({'features': X_FeatureAnalysis.columns, 'importance':Importances}) # This creates a Dataframe of features and their importances\nfeature_importances = feature_importances.sort_values(by='importance', ascending = False)\nfeature_importances","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:33:51.569860Z","iopub.execute_input":"2025-06-13T06:33:51.570213Z","iopub.status.idle":"2025-06-13T06:33:51.592380Z","shell.execute_reply.started":"2025-06-13T06:33:51.570190Z","shell.execute_reply":"2025-06-13T06:33:51.591142Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"copy_balanced.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:33:51.593678Z","iopub.execute_input":"2025-06-13T06:33:51.594029Z","iopub.status.idle":"2025-06-13T06:33:51.618808Z","shell.execute_reply.started":"2025-06-13T06:33:51.593998Z","shell.execute_reply":"2025-06-13T06:33:51.617753Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Training.\n\n- Three alogorithims will be used (Logistic regression, randomForest classifier and XGBclassifier) then the best performing model selected.\n\n**Before then data will be devided to a train and test set.**","metadata":{}},{"cell_type":"code","source":"# 1. Select X (features) and y (target) from the DataFrame 'copy_balanced'\n\nx = copy_balanced.drop('Outcome', axis=1)  # X contains all columns except the target\ny = copy_balanced['Outcome']\n\n# Here, test_size=0.2 means 20% test data, 80% train data. Set random_state for reproducibility.\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:33:51.619913Z","iopub.execute_input":"2025-06-13T06:33:51.620258Z","iopub.status.idle":"2025-06-13T06:33:51.649160Z","shell.execute_reply.started":"2025-06-13T06:33:51.620228Z","shell.execute_reply":"2025-06-13T06:33:51.647741Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_train.shape, x_test.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:33:51.651500Z","iopub.execute_input":"2025-06-13T06:33:51.651840Z","iopub.status.idle":"2025-06-13T06:33:51.681361Z","shell.execute_reply.started":"2025-06-13T06:33:51.651816Z","shell.execute_reply":"2025-06-13T06:33:51.680010Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_train.shape, y_test.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:33:51.682844Z","iopub.execute_input":"2025-06-13T06:33:51.683175Z","iopub.status.idle":"2025-06-13T06:33:51.714718Z","shell.execute_reply.started":"2025-06-13T06:33:51.683136Z","shell.execute_reply":"2025-06-13T06:33:51.713066Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1. Logistic Regression Model","metadata":{}},{"cell_type":"code","source":"# Initialize the logistic regression model\nlog_model = LogisticRegression(max_iter =1000, random_state=42)\n\n# Train the model on the training data\nlog_model.fit(x_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:33:51.716311Z","iopub.execute_input":"2025-06-13T06:33:51.716805Z","iopub.status.idle":"2025-06-13T06:33:52.337202Z","shell.execute_reply.started":"2025-06-13T06:33:51.716767Z","shell.execute_reply":"2025-06-13T06:33:52.334859Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict labels for the test set\nLog_ypredict = log_model.predict(x_test)\nLog_ypredict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:33:52.337932Z","iopub.execute_input":"2025-06-13T06:33:52.338216Z","iopub.status.idle":"2025-06-13T06:33:52.351480Z","shell.execute_reply.started":"2025-06-13T06:33:52.338192Z","shell.execute_reply":"2025-06-13T06:33:52.350757Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate the model\n# Get accuracy\naccuracy = accuracy_score(y_test, Log_ypredict)\n\n# Get classification report \nreport = classification_report(y_test, Log_ypredict, output_dict=True)\n\n# Convert classification report to DataFrame\nC_Report  = pd.DataFrame(report).transpose()\n\n# Add accuracy as a new row\nC_Report.loc['accuracy'] = [accuracy, None, None, None]\n## Print Report\nC_Report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:33:52.354213Z","iopub.execute_input":"2025-06-13T06:33:52.354585Z","iopub.status.idle":"2025-06-13T06:33:52.405925Z","shell.execute_reply.started":"2025-06-13T06:33:52.354559Z","shell.execute_reply":"2025-06-13T06:33:52.404426Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The regression model predicts diabetes with **78.5% accuracy**. It performs similarly for both classes:\n\n* **Non-diabetic (0)**: F1-score = **77.3%**\n* **Diabetic (1)**: F1-score = **79.6%**\n\nPrecision and recall are well balanced, indicating the model reliably distinguishes between diabetic and non-diabetic cases without favoring either class.\n","metadata":{}},{"cell_type":"code","source":"# Logistic Regression confusion matrix\nlog_cm = confusion_matrix(y_test, Log_ypredict)\nplt.figure(figsize=(5,4))\nsns.heatmap(log_cm, annot=True, fmt='d', cmap='Blues')\nplt.title('Logistic Regression Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.savefig(\"Logistic Regression Matrix\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:33:52.407690Z","iopub.execute_input":"2025-06-13T06:33:52.409617Z","iopub.status.idle":"2025-06-13T06:33:52.708890Z","shell.execute_reply.started":"2025-06-13T06:33:52.409582Z","shell.execute_reply":"2025-06-13T06:33:52.707724Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Explanation\n\n**True Negatives (75):** The model correctly predicted 75 non-diabetic individuals.\n\n**False Positives (22):** The model incorrectly labeled 22 non-diabetic individuals as diabetic.\n\n**False Negatives (21):** The model missed 21 diabetic individuals.\n\n**True Positives (82):** The model correctly identified 82 diabetic individuals.","metadata":{}},{"cell_type":"markdown","source":"## 2. Random Forest Tree Model","metadata":{}},{"cell_type":"code","source":"# Initialize and train the Random Forest model\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(x_train, y_train)\n\n# Predict on the test set\nrf_ypredict = rf_model.predict(x_test)\n\n# Calculate accuracy\nrf_accuracy = accuracy_score(y_test, rf_ypredict)\n\n# Get classification report\nrf_report = classification_report(y_test, rf_ypredict, output_dict=True)\nRF_report = pd.DataFrame(rf_report).transpose()\n\n# Add accuracy to the table\nRF_report.loc['accuracy'] = [rf_accuracy, None, None, None]\n\n# Display the evaluation table\nRF_report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:33:52.710444Z","iopub.execute_input":"2025-06-13T06:33:52.711135Z","iopub.status.idle":"2025-06-13T06:33:53.006351Z","shell.execute_reply.started":"2025-06-13T06:33:52.711075Z","shell.execute_reply":"2025-06-13T06:33:53.005137Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The **Random Forest model** demonstrates strong performance in predicting diabetes, achieving an overall **accuracy of 90.5%**. It shows a well-balanced ability to classify both diabetic and non-diabetic individuals.\n\nFor non-diabetic cases (class 0), the model has a **precision of 91.3%**, meaning it correctly identifies non-diabetics in most predictions, and a **recall of 88.5%**, indicating that it successfully detects a majority of actual non-diabetic individuals. The resulting **F1-score is 89.9%**, reflecting a good balance between precision and recall for this class.\n\nFor diabetic cases (class 1), the model yields a **precision of 89.7%** and a higher **recall of 92.3%**, suggesting that it is particularly effective at capturing actual diabetes cases. The **F1-score for this class is 90.9%**, indicating strong overall performance.\n\nThe **macro average** scores—precision (90.5%), recall (90.4%), and F1-score (90.4%)—show consistent performance across both classes without favoring one. Similarly, the **weighted averages**, which account for class distribution, mirror the overall performance closely.\n\nIn summary, the Random Forest model is accurate, balanced, and particularly effective at identifying diabetic individuals, making it a reliable tool for diabetes prediction.\n","metadata":{}},{"cell_type":"code","source":"# Random Forest confusion matrix\nrf_cm = confusion_matrix(y_test, rf_ypredict)\nplt.figure(figsize=(5,4))\nsns.heatmap(rf_cm, annot=True, fmt='d', cmap='Greens')\nplt.title('Random Forest Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.savefig(\"RF Model\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:33:53.007491Z","iopub.execute_input":"2025-06-13T06:33:53.007947Z","iopub.status.idle":"2025-06-13T06:33:53.353352Z","shell.execute_reply.started":"2025-06-13T06:33:53.007910Z","shell.execute_reply":"2025-06-13T06:33:53.352329Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The Random Forest model demonstrates strong classification performance, as shown in the confusion matrix:\n\n* It correctly identified **85 out of 96 non-diabetic cases** (True Negatives) and misclassified **11** as diabetic (False Positives).\n* It correctly identified **96 out of 104 diabetic cases** (True Positives) and misclassified **8** as non-diabetic (False Negatives).\n","metadata":{}},{"cell_type":"markdown","source":"## 3. XGBClassification Model","metadata":{}},{"cell_type":"code","source":"#Initialize and train the XGBoost model\nxgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\nxgb_model.fit(x_train, y_train)\n\n#Predict on the test set\nxgb_ypredict = xgb_model.predict(x_test)\n\n#Calculate accuracy\nxgb_accuracy = accuracy_score(y_test, xgb_ypredict)\n\n# Get classification report\nreport = classification_report(y_test, xgb_ypredict, output_dict=True)\nxgb_report = pd.DataFrame(report).transpose()\n\n# Add accuracy to the table\nxgb_report.loc['accuracy'] = [xgb_accuracy, None, None, None]\n\n# Display the evaluation table\nxgb_report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:33:53.354438Z","iopub.execute_input":"2025-06-13T06:33:53.355455Z","iopub.status.idle":"2025-06-13T06:33:53.479346Z","shell.execute_reply.started":"2025-06-13T06:33:53.355423Z","shell.execute_reply":"2025-06-13T06:33:53.478288Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The **XGBoost (XGBClassifier) model** achieved an overall **accuracy of 88.0%** in predicting diabetes, indicating strong and consistent performance. For the **non-diabetic class (0)**, it recorded a **precision of 90%**, **recall of 84.3%**, and an **F1-score of 87.09%**, showing it is effective at correctly identifying non-diabetic individuals but slightly less so at capturing all actual cases. For the **diabetic class (1)**, the model achieved a **precision of 86.3%**, a higher **recall of 91.3%**, and an **F1-score of 88.7%**, suggesting it performs particularly well in identifying most individuals with diabetes.\n\nThe **macro average scores**—**precision: 88.18%**, **recall: 87.8%**, and **F1-score: 87.9%**—show that the model maintains a balanced performance across both classes. The **weighted averages**—**precision: 88.1%**, **recall: 88.0%**, and **F1-score: 87.9%**—reflect the model’s performance accounting for the class distribution, further confirming its overall reliability. With strong precision, high recall for diabetic cases, and balanced F1-scores, the XGBoost model demonstrates solid potential for supporting diabetes risk identification in healthcare settings.\n","metadata":{}},{"cell_type":"code","source":"# Begion ny computing the confusion matrix\nxgb_cm = confusion_matrix(y_test, xgb_ypredict)\n\n# Plot the confusion matrix\nplt.figure(figsize=(5,4))\nsns.heatmap(xgb_cm, annot=True, fmt='d', cmap='Oranges')\nplt.title('XGBoost Classifier Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.savefig(\"XGBC matrix\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:33:53.480513Z","iopub.execute_input":"2025-06-13T06:33:53.480845Z","iopub.status.idle":"2025-06-13T06:33:53.799751Z","shell.execute_reply.started":"2025-06-13T06:33:53.480822Z","shell.execute_reply":"2025-06-13T06:33:53.798361Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The XGBoost Classifier demonstrates high predictive performance in identifying diabetes cases, as shown in the confusion matrix:\n\n- It correctly classified **81 out of 96 non-diabetic cases (True Negatives)**, while 15 were incorrectly labeled as diabetic (False Positives).\n\n- It correctly identified **95 out of 104 diabetic cases (True Positives)**, with only 9 missed as non-diabetic (False Negatives).","metadata":{}},{"cell_type":"markdown","source":"## Conclusion\n\n* **Random Forest outperformed all models**, achieving the highest **accuracy (90.5%)** and strong, balanced **F1-scores** for both classes (non-diabetic: 89.9%, diabetic: 90.9%).\n\n* **XGBoost followed closely** with **88.0% accuracy**, showing high recall for diabetic cases (91.3%) and solid overall performance, making it a reliable alternative.\n\n* **Logistic Regression lagged**, with lower **accuracy (78.5%)** and moderate F1-scores, though it maintained a balanced classification between classes.\n\n* **Conclusion**: **Random Forest** is the most suitable model for predicting diabetes due to its superior and consistent performance across all evaluation metrics.\n","metadata":{}}]}